{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Generate the Random Points and Bounding Boxes using https://www.jasondavies.com/poisson-disc/ algorithm\n",
    "def generate_random_bounding_boxes(image_width, image_height, number_boxes, min_distance, box_percentage):\n",
    "    def generate_random_points(image_width, image_height, min_distance, num_points, k=30):\n",
    "        image_size = (image_width, image_height)\n",
    "        cell_size = min_distance / np.sqrt(2)\n",
    "        grid_width = int(np.ceil(image_width / cell_size))\n",
    "        grid_height = int(np.ceil(image_height / cell_size))\n",
    "        grid = np.empty((grid_width, grid_height), dtype=np.int32)\n",
    "        grid.fill(-1)\n",
    "\n",
    "        points = []\n",
    "        active_points = []\n",
    "\n",
    "        def generate_random_point():\n",
    "            return np.random.uniform(0, image_width), np.random.uniform(0, image_height)\n",
    "\n",
    "        def get_neighboring_cells(point):\n",
    "            x, y = point\n",
    "            x_index = int(x / cell_size)\n",
    "            y_index = int(y / cell_size)\n",
    "\n",
    "            cells = []\n",
    "            for i in range(max(0, x_index - 2), min(grid_width, x_index + 3)):\n",
    "                for j in range(max(0, y_index - 2), min(grid_height, y_index + 3)):\n",
    "                    cells.append((i, j))\n",
    "\n",
    "            return cells\n",
    "\n",
    "        def is_point_valid(point):\n",
    "            x, y = point\n",
    "            if x < 0 or y < 0 or x >= image_width or y >= image_height:\n",
    "                return False\n",
    "\n",
    "            x_index = int(x / cell_size)\n",
    "            y_index = int(y / cell_size)\n",
    "\n",
    "            cells = get_neighboring_cells(point)\n",
    "            for cell in cells:\n",
    "                if grid[cell] != -1:\n",
    "                    cell_points = points[grid[cell]]\n",
    "                    if np.any(np.linalg.norm(np.array(cell_points) - np.array(point), axis=None) < min_distance):\n",
    "                        return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        def add_point(point):\n",
    "            x, y = point\n",
    "            x_index = int(x / cell_size)\n",
    "            y_index = int(y / cell_size)\n",
    "\n",
    "            points.append(point)\n",
    "            index = len(points) - 1\n",
    "            grid[x_index, y_index] = index\n",
    "            active_points.append(point)\n",
    "\n",
    "        start_point = generate_random_point()\n",
    "        add_point(start_point)\n",
    "\n",
    "        while active_points and len(points) < num_points:\n",
    "            random_index = np.random.randint(len(active_points))\n",
    "            random_point = active_points[random_index]\n",
    "            added_new_point = False\n",
    "\n",
    "            for _ in range(k):\n",
    "                angle = 2 * np.pi * np.random.random()\n",
    "                radius = min_distance + min_distance * np.random.random()\n",
    "                new_point = (random_point[0] + radius * np.cos(angle), random_point[1] + radius * np.sin(angle))\n",
    "                if is_point_valid(new_point):\n",
    "                    add_point(new_point)\n",
    "                    added_new_point = True\n",
    "\n",
    "            if not added_new_point:\n",
    "                active_points.pop(random_index)\n",
    "\n",
    "        return points\n",
    "    \n",
    "\n",
    "    points = generate_random_points(image_width, image_height, min_distance, number_boxes)\n",
    "    \n",
    "    \n",
    "    box_width = int(image_width * box_percentage)\n",
    "    box_height = int(image_height * box_percentage)\n",
    "\n",
    "    bounding_boxes = []\n",
    "    for point in points:\n",
    "        x = int(point[0] - box_width / 2)\n",
    "        y = int(point[1] - box_height / 2)\n",
    "\n",
    "        # Adjust the coordinates to keep the bounding box within the image\n",
    "        x = max(0, min(x, image_width - box_width))\n",
    "        y = max(0, min(y, image_height - box_height))\n",
    "\n",
    "        bounding_boxes.append([x, y, x+box_width, y+box_height])\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bounding_boxes(image_width, image_height, box_size):\n",
    "    stride = int(0.6 * box_size)\n",
    "    num_boxes_horizontal = (image_width - box_size) // stride + 1\n",
    "    num_boxes_vertical = (image_height - box_size) // stride + 1\n",
    "\n",
    "    bounding_boxes = []\n",
    "    for i in range(num_boxes_horizontal):\n",
    "        for j in range(num_boxes_vertical):\n",
    "            start_x = i * stride\n",
    "            start_y = j * stride\n",
    "            end_x = start_x + box_size\n",
    "            end_y = start_y + box_size\n",
    "\n",
    "            # Adjust for boxes extending beyond image boundaries\n",
    "            end_x = min(end_x, image_width)\n",
    "            end_y = min(end_y, image_height)\n",
    "\n",
    "            bounding_boxes.append((start_x, start_y, end_x, end_y))\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read image using PIL\n",
    "image = Image.open('4389.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox = generate_random_bounding_boxes(image.size[0], image.size[1], 50, image.size[0]/5, 0.25)\n",
    "bbox = generate_bounding_boxes(image.size[0], image.size[1], int(image.size[0]*0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.segment_anything.utils.transforms import ResizeLongestSide\n",
    "from modeling.segment_anything import prepare_sam\n",
    "from config import cfg  # Import the default config file\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import threshold, normalize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBOX:\n",
      "  BOX_LIMITER: 100\n",
      "  MIN_DISTANCE: 50\n",
      "  NUMBER: 30\n",
      "  SIZE_REF: 0.25\n",
      "DATALOADER:\n",
      "  NUM_WORKERS: 8\n",
      "  TRAIN_DATA: 0.8\n",
      "  VALID_DATA: 0.2\n",
      "DATASETS:\n",
      "  ROOT_DIR: /home/aghosh57/Kerner-Lab/all_dataset/\n",
      "INPUT:\n",
      "  \n",
      "LOGGER:\n",
      "  LEVEL: INFO\n",
      "LOSS:\n",
      "  DICE_LOSS_WEIGHT: 1\n",
      "  FOCAL_LOSS_WEIGHT: 5\n",
      "MASKS:\n",
      "  MIN_AREA: 50\n",
      "MODEL:\n",
      "  CHECKPOINT: /home/aghosh57/Kerner-Lab/SAM-FineTuning/logs/Jun26_22-55-27/model_checkpoints/sam_checkpoint_1.pth\n",
      "  DEVICE: cuda\n",
      "  SAVE_INTERVAL: 1\n",
      "  TYPE: base\n",
      "OUTPUT_DIR: ./logs/\n",
      "SOLVER:\n",
      "  ITEMS_PER_BATCH: 1\n",
      "  MAX_EPOCHS: 1\n",
      "  MIN_LR: 1e-06\n",
      "  START_LR: 0.01\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "TEST:\n",
      "  ITEMS_PER_BATCH: 6\n",
      "VALID:\n",
      "  ITEMS_PER_BATCH: 8\n"
     ]
    }
   ],
   "source": [
    "cfg.freeze()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model and download the checkpoint if needed\n",
    "model = prepare_sam(checkpoint=cfg.MODEL.CHECKPOINT, model_type = cfg.MODEL.TYPE)\n",
    "device = cfg.MODEL.DEVICE\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the portion of the model to be trained (We will train only the mask_decoder part)\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image needs to be resized to 1024*1024 and necessary preprocessing should be done\n",
    "# image = cv2.imread('4389.png')\n",
    "scale_factor = 1024 / max(image.size[0], image.size[1])\n",
    "\n",
    "sam_transform = ResizeLongestSide(model.image_encoder.img_size)\n",
    "resize_img = sam_transform.apply_image(np.array(image))\n",
    "resize_img = torch.as_tensor(resize_img.transpose(2, 0, 1)).to(device)\n",
    "resize_img = model.preprocess(resize_img[None,:,:,:]) # (1, 3, 1024, 1024)\n",
    "\n",
    "#scale the bbox prompts and point prompts according to the scale factor\n",
    "bbox_prompts = np.around(np.array(bbox) * scale_factor)\n",
    "\n",
    "bbox_prompts = torch.as_tensor(bbox_prompts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_embeddings = model.image_encoder(resize_img)  # (B,256,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "    points=None,\n",
    "    boxes=bbox_prompts,\n",
    "    masks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_masks, iou_predictions = model.mask_decoder(\n",
    "    image_embeddings=image_embeddings.to(device),  # (B, 256, 64, 64)\n",
    "    image_pe=model.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "    sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "    dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409],\n",
       "        [0.5409]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-54420.0977, -56006.5117, -32643.9688,  ..., -56006.5117,\n",
       "           -32643.8242, -37241.1289],\n",
       "          [-16110.6406, -33661.3164,  -2513.8547,  ..., -33661.3164,\n",
       "            -2513.8315, -18808.5117],\n",
       "          [-51992.5039, -56582.9531, -13255.1260,  ..., -56582.7109,\n",
       "           -13254.9395, -13576.3770],\n",
       "          ...,\n",
       "          [-16110.6406, -33661.3164,  -2513.8682,  ..., -33661.2227,\n",
       "            -2513.6069, -18808.0977],\n",
       "          [-51992.5000, -56582.9492, -13254.7637,  ..., -56582.5273,\n",
       "           -13254.4746, -13575.9336],\n",
       "          [ -3193.0127, -31927.9219,  -9549.2617,  ..., -31927.8945,\n",
       "            -9549.1865, -16216.1758]]],\n",
       "\n",
       "\n",
       "        [[[-54420.0234, -56006.4258, -32643.9199,  ..., -56006.4258,\n",
       "           -32643.7305, -37241.0664],\n",
       "          [-16110.6172, -33661.2695,  -2513.8540,  ..., -33661.2695,\n",
       "            -2513.7273, -18808.4629],\n",
       "          [-51992.4297, -56582.8867, -13255.1289,  ..., -56582.8242,\n",
       "           -13254.8418, -13576.2676],\n",
       "          ...,\n",
       "          [-16110.5996, -33661.2617,  -2513.7490,  ..., -33661.1914,\n",
       "            -2513.4360, -18807.9395],\n",
       "          [-51992.3672, -56582.6680, -13254.8135,  ..., -56582.3555,\n",
       "           -13254.7061, -13576.1523],\n",
       "          [ -3192.8120, -31927.9277,  -9549.1367,  ..., -31927.7891,\n",
       "            -9548.8730, -16215.9512]]],\n",
       "\n",
       "\n",
       "        [[[-54420.0547, -56006.4766, -32643.9434,  ..., -56006.4766,\n",
       "           -32643.7344, -37241.4492],\n",
       "          [-16110.6309, -33661.2930,  -2513.8562,  ..., -33661.2930,\n",
       "            -2513.8262, -18808.5176],\n",
       "          [-51992.4727, -56582.9180, -13255.0938,  ..., -56582.6758,\n",
       "           -13254.9531, -13576.3887],\n",
       "          ...,\n",
       "          [-16110.6309, -33661.2930,  -2513.8540,  ..., -33661.2031,\n",
       "            -2513.3730, -18806.9180],\n",
       "          [-51992.4297, -56582.8867, -13254.8770,  ..., -56582.4570,\n",
       "           -13254.4609, -13575.9180],\n",
       "          [ -3193.0110, -31927.8926,  -9549.1758,  ..., -31927.8633,\n",
       "            -9549.1777, -16216.1611]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-54420.1055, -56006.5234, -32644.0566,  ..., -56006.5234,\n",
       "           -32643.8340, -37241.1367],\n",
       "          [-16110.6436, -33661.3242,  -2513.8640,  ..., -33661.3242,\n",
       "            -2513.8325, -18808.5156],\n",
       "          [-51992.5156, -56582.9727, -13255.1504,  ..., -56582.7227,\n",
       "           -13254.9434, -13576.3809],\n",
       "          ...,\n",
       "          [-16110.6436, -33661.3242,  -2513.7573,  ..., -33661.2305,\n",
       "            -2513.6077, -18808.1016],\n",
       "          [-51992.4766, -56582.7891, -13254.8555,  ..., -56582.6875,\n",
       "           -13254.4795, -13575.9375],\n",
       "          [ -3192.8169, -31927.9863,  -9549.1768,  ..., -31927.9141,\n",
       "            -9549.1904, -16216.1797]]],\n",
       "\n",
       "\n",
       "        [[[-54420.1562, -56006.5742, -32644.0449,  ..., -56006.5742,\n",
       "           -32643.7500, -37241.4961],\n",
       "          [-16110.6592, -33661.3555,  -2513.8525,  ..., -33661.3555,\n",
       "            -2513.7319, -18808.4707],\n",
       "          [-51992.5078, -56582.9766, -13255.1426,  ..., -56582.8125,\n",
       "           -13254.8398, -13576.2451],\n",
       "          ...,\n",
       "          [-16110.6592, -33661.3555,  -2513.8586,  ..., -33661.2656,\n",
       "            -2513.8574, -18808.5703],\n",
       "          [-51992.5586, -56583.0273, -13254.7793,  ..., -56582.6992,\n",
       "           -13254.5127, -13575.9717],\n",
       "          [ -3193.0161, -31927.9648,  -9549.2734,  ..., -31927.9160,\n",
       "            -9549.1924, -16216.1943]]],\n",
       "\n",
       "\n",
       "        [[[-54420.1016, -56006.5156, -32644.0234,  ..., -56006.5156,\n",
       "           -32643.8008, -37241.1016],\n",
       "          [-16110.6416, -33661.3242,  -2513.8472,  ..., -33661.3242,\n",
       "            -2513.8411, -18808.5195],\n",
       "          [-51992.4453, -56582.9141, -13255.1504,  ..., -56582.7188,\n",
       "           -13254.9658, -13576.3984],\n",
       "          ...,\n",
       "          [-16110.6416, -33661.3242,  -2513.7437,  ..., -33661.2383,\n",
       "            -2513.4634, -18807.9395],\n",
       "          [-51992.4453, -56582.7500, -13254.8545,  ..., -56582.4961,\n",
       "           -13255.0742, -13576.5527],\n",
       "          [ -3192.8167, -31927.9727,  -9549.1758,  ..., -31927.8926,\n",
       "            -9548.8389, -16216.1211]]]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_res_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaled_masks = model.postprocess_masks(low_res_masks, (1024, 1024), max(image.size[0], image.size[1])).to(device)\n",
    "high_res_masks = normalize(threshold(upscaled_masks, 0.0, 0)).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-54420.0977, -54986.6758, -55224.9219,  ..., -33721.0078,\n",
       "           -35599.2852, -37241.1289],\n",
       "          [-40738.1445, -43340.9883, -46610.0469,  ..., -23445.0781,\n",
       "           -27524.2402, -30658.0508],\n",
       "          [-20834.2969, -26418.4453, -34111.7031,  ...,  -8028.7134,\n",
       "           -15222.4521, -20478.7109],\n",
       "          ...,\n",
       "          [-46238.4883, -48848.8477, -51732.9180,  ..., -13834.1641,\n",
       "           -13485.2227, -13975.3730],\n",
       "          [-20620.8691, -27803.7812, -37957.8867,  ..., -12104.3496,\n",
       "           -13701.5957, -15273.2598],\n",
       "          [ -3193.0127, -13455.4814, -28506.3164,  ..., -10803.3076,\n",
       "           -13835.1807, -16216.1758]]],\n",
       "\n",
       "\n",
       "        [[[-54420.0234, -54986.5977, -55224.8359,  ..., -33720.9180,\n",
       "           -35599.2109, -37241.0664],\n",
       "          [-40738.0898, -43340.9219, -46609.9805,  ..., -23444.9863,\n",
       "           -27524.1660, -30657.9922],\n",
       "          [-20834.2676, -26418.4082, -34111.6523,  ...,  -8028.6167,\n",
       "           -15222.3809, -20478.6582],\n",
       "          ...,\n",
       "          [-46238.3477, -48848.6719, -51732.6914,  ..., -13834.3213,\n",
       "           -13485.3877, -13975.5381],\n",
       "          [-20620.6914, -27803.6348, -37957.7812,  ..., -12104.2363,\n",
       "           -13701.5098, -15273.1953],\n",
       "          [ -3192.8120, -13455.3555, -28506.2969,  ..., -10803.0078,\n",
       "           -13834.9248, -16215.9512]]],\n",
       "\n",
       "\n",
       "        [[[-54420.0547, -54986.6367, -55224.8867,  ..., -33720.9609,\n",
       "           -35599.4570, -37241.4492],\n",
       "          [-40738.1133, -43340.9492, -46610.0195,  ..., -23445.0469,\n",
       "           -27524.3496, -30658.2578],\n",
       "          [-20834.2852, -26418.4297, -34111.6758,  ...,  -8028.7041,\n",
       "           -15222.4707, -20478.7480],\n",
       "          ...,\n",
       "          [-46238.4258, -48848.7891, -51732.8672,  ..., -13834.1406,\n",
       "           -13485.1855, -13975.3271],\n",
       "          [-20620.8438, -27803.7520, -37957.8477,  ..., -12104.3379,\n",
       "           -13701.5820, -15273.2461],\n",
       "          [ -3193.0110, -13455.4707, -28506.2871,  ..., -10803.2969,\n",
       "           -13835.1680, -16216.1611]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-54420.1055, -54986.6836, -55224.9336,  ..., -33721.0195,\n",
       "           -35599.2930, -37241.1367],\n",
       "          [-40738.1562, -43340.9883, -46610.0586,  ..., -23445.0879,\n",
       "           -27524.2461, -30658.0566],\n",
       "          [-20834.3008, -26418.4492, -34111.7070,  ...,  -8028.7153,\n",
       "           -15222.4561, -20478.7148],\n",
       "          ...,\n",
       "          [-46238.4492, -48848.7734, -51732.8008,  ..., -13834.1729,\n",
       "           -13485.2266, -13975.3770],\n",
       "          [-20620.7344, -27803.6914, -37957.8594,  ..., -12104.3555,\n",
       "           -13701.5996, -15273.2646],\n",
       "          [ -3192.8169, -13455.3789, -28506.3496,  ..., -10803.3115,\n",
       "           -13835.1846, -16216.1797]]],\n",
       "\n",
       "\n",
       "        [[[-54420.1562, -54986.7344, -55224.9844,  ..., -33720.9805,\n",
       "           -35599.4961, -37241.4961],\n",
       "          [-40738.1875, -43341.0312, -46610.1016,  ..., -23445.0273,\n",
       "           -27524.3535, -30658.2715],\n",
       "          [-20834.3203, -26418.4727, -34111.7383,  ...,  -8028.6284,\n",
       "           -15222.4150, -20478.7090],\n",
       "          ...,\n",
       "          [-46238.5430, -48848.9023, -51732.9883,  ..., -13834.2080,\n",
       "           -13485.2676, -13975.4219],\n",
       "          [-20620.8906, -27803.8164, -37957.9375,  ..., -12104.3691,\n",
       "           -13701.6172, -15273.2861],\n",
       "          [ -3193.0161, -13455.4990, -28506.3535,  ..., -10803.3145,\n",
       "           -13835.1943, -16216.1943]]],\n",
       "\n",
       "\n",
       "        [[[-54420.1016, -54986.6797, -55224.9258,  ..., -33720.9883,\n",
       "           -35599.2578, -37241.1016],\n",
       "          [-40738.1484, -43340.9883, -46610.0547,  ..., -23445.0703,\n",
       "           -27524.2246, -30658.0352],\n",
       "          [-20834.2969, -26418.4453, -34111.7031,  ...,  -8028.7197,\n",
       "           -15222.4570, -20478.7168],\n",
       "          ...,\n",
       "          [-46238.4219, -48848.7422, -51732.7656,  ..., -13834.6406,\n",
       "           -13485.7373, -13975.9062],\n",
       "          [-20620.7246, -27803.6738, -37957.8398,  ..., -12104.3594,\n",
       "           -13701.7119, -15273.4473],\n",
       "          [ -3192.8167, -13455.3750, -28506.3379,  ..., -10802.9971,\n",
       "           -13835.0215, -16216.1211]]]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upscaled_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_res_masks = np.squeeze(high_res_masks.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the masks to get the final mask\n",
    "final_mask = np.stack(high_res_masks)\n",
    "final_mask = np.sum(final_mask, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the bounding boxes on the image\n",
    "for box in bbox:\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.rectangle(box, outline='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_res_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[1].imshow(final_mask)\n",
    "ax[1].set_title(\"Full Predicted Image\")\n",
    "ax[2].imshow(high_res_masks[21])\n",
    "ax[2].set_title(\"Predicted Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the masks and the image (Only For Visualization Purposes)\n",
    "for i in range(len(high_res_masks)):\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].imshow(final_mask)\n",
    "    ax[1].set_title(\"Full Predicted Image\")\n",
    "    ax[2].imshow(high_res_masks[i])\n",
    "    ax[2].set_title(\"Predicted Mask\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
